{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ed7413-c876-4d38-b080-79c71133549b",
   "metadata": {},
   "source": [
    "# Manage Conversation History\n",
    "\n",
    "One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. In order to prevent this from happening, you need to properly manage the conversation history.\n",
    "\n",
    "---\n",
    "\n",
    "Message history can grow quickly and exceed LLM context window size, whether you're building chatbots with many conversation turns or agentic systems with numerous tool calls. There are several strategies for managing the message history:\n",
    "\n",
    "* [message trimming](#keep-the-original-message-history-unmodified) — remove first or last N messages in the history\n",
    "* [summarization](#summarizing-message-history) — summarize earlier messages in the history and replace them with a summary\n",
    "* custom strategies (e.g., message filtering, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "Long conversations can exceed the LLM's context window. Common solutions are:\n",
    "\n",
    "* [Summarization](#summarize-message-history): Maintain a running summary of the conversation\n",
    "* [Trimming](#trim-message-history): Remove first or last N messages in the history\n",
    "\n",
    "This allows the agent to keep track of the conversation without exceeding the LLM's context window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd446a-808f-4394-be92-d45ab818953c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the packages we're going to want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe11f4-62ed-4dc4-8875-3db21e260d1d",
   "metadata": {},
   "source": [
    "Next, we need to set API keys for Anthropic (the LLM we will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ANTHROPIC_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed46a8-effe-4596-b0e1-a6a29ee16f5c",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c338d33-c17f-40a5-8167-a9ee4aaeeea7",
   "metadata": {},
   "source": [
    "## Processing Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f6b34-33cf-4d19-afce-ab1a3dee7cd0",
   "metadata": {},
   "source": [
    "### Use inside a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413486d5-17d9-4c4e-b062-783c84017637",
   "metadata": {},
   "outputs": [],
   "source": [
    "To process message in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a9480-15a9-4eb4-8dce-227b88cc3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "    \n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = filter_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0adba-7ca4-444b-beed-4db072b7cfcc",
   "metadata": {},
   "source": [
    "??? example \"Full example: process messages in `call_model` node\"\n",
    "\n",
    "    ```python\n",
    "    from langchain.chat_models import init_chat_model\n",
    "    from langchain_core.messages import AnyMessage\n",
    "    \n",
    "    from langgraph.graph import MessagesState, StateGraph, START\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    \n",
    "    def process_messages(messages: list[AnyMessage]):\n",
    "        # keep only the last message\n",
    "        return messages[-1:]\n",
    "    \n",
    "    def call_model(state: MessagesState):\n",
    "        # highlight-next-line\n",
    "        messages = process_messages(state[\"messages\"])\n",
    "        response = model.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "    \n",
    "    \n",
    "    # highight-next-line\n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    # highight-next-line\n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the process_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345fa93-4181-4eec-8dc9-0969969fa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide-cell\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def process_messages(messages: list[AnyMessage]):\n",
    "    # keep only the last message\n",
    "    return messages[-1:]\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # highlight-next-line\n",
    "    messages = process_messages(state[\"messages\"])\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# highight-next-line\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "# highight-next-line\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the process_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae813e-0fbb-4454-bfde-1238d6496db5",
   "metadata": {},
   "source": [
    "### Use as a separate node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be1f8b-b3d7-4dc0-9668-6c73cf9353f5",
   "metadata": {},
   "source": [
    "Use this if you need to preprocess inputs to the LLM **and** update the graph state. This is useful when for summarizing message history, where `process_messages` step needs to return updated messages and running summary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c2b08-be2f-4885-9166-b08126118ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "        \"some_key\": \"some_value\"  # (2)!\n",
    "    }\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):  # (3)!\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(OverallState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ed02d-858c-40e1-8387-d53940c08ac5",
   "metadata": {},
   "source": [
    "1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "2. `some_key` will be added to the **overall** graph state.\n",
    "3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6181647a-9e36-49b4-9eaa-e133e1550252",
   "metadata": {},
   "source": [
    "??? example \"Full example: process messages in a separate node\"\n",
    "\n",
    "    ```python\n",
    "    from typing_extensions import TypedDict\n",
    "    \n",
    "    class OverallState(MessagesState):\n",
    "        some_key: str\n",
    "    \n",
    "    \n",
    "    class LLMInputState(TypedDict):\n",
    "        # highlight-next-line\n",
    "        llm_input_messages: list[AnyMessage]\n",
    "    \n",
    "    \n",
    "    def process_messages(state: MessagesState):\n",
    "        return {\n",
    "            # keep only the last message\n",
    "            \"llm_input_messages\": state[\"messages\"][-1:],  # (1)!\n",
    "            \"some_key\": \"some_value\"  # (2)!\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # highlight-next-line\n",
    "    def call_model(state: LLMInputState):  # (3)!\n",
    "        response = model.invoke(state[\"llm_input_messages\"])\n",
    "        return {\"messages\": response}\n",
    "    \n",
    "    \n",
    "    checkpointer = InMemorySaver()\n",
    "    model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "    \n",
    "    builder = StateGraph(OverallState)\n",
    "    # highlight-next-line\n",
    "    builder.add_node(process_messages)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"process_messages\")\n",
    "    builder.add_edge(\"process_messages\", \"call_model\")\n",
    "    \n",
    "    app = builder.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # This will now not remember the previous messages\n",
    "    # because we set `messages[-1:]` in the filter_messages function\n",
    "    for event in app.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        print(event)\n",
    "        print(\"\\n\")\n",
    "    ```\n",
    "    \n",
    "    1. `llm_input_messages` will be visible only to the `call_model` node and won't be added to overall graph state.\n",
    "    2. `some_key` will be added to the **overall** graph state.\n",
    "    3. `call_model` uses a private input schema, different from the `OverallState`. This allows us to change the input to `call_model` node without affecting the **overall** graph state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78faa1f2-6ae5-406d-baa3-9f1e00e87d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='6eb8c38a-7f7c-4c7e-98ab-2e01e3e860a9')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?', additional_kwargs={}, response_metadata={'id': 'msg_01SoZtnvMLsSJCQdL6ajXFK8', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 21}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-d8998b94-4f49-494b-9b91-85f676cba22a-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n",
      "{'process_messages': {'llm_input_messages': [HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='9a38c63e-f23d-4c38-a06d-7d7c74ade462')], 'some_key': 'some_value'}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content=\"I apologize, but I don't know your name. I can't automatically know personal details about you unless you've previously told me. Could you introduce yourself?\", additional_kwargs={}, response_metadata={'id': 'msg_01EZbuZrc65PKy8gGF4TcB3i', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 36}, 'model_name': 'claude-3-5-haiku-20241022'}, id='run-b375691b-078a-4a87-a642-8bf72a96c198-0', usage_metadata={'input_tokens': 12, 'output_tokens': 36, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hide-cell\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class OverallState(MessagesState):\n",
    "    some_key: str\n",
    "\n",
    "\n",
    "class LLMInputState(TypedDict):\n",
    "    # highlight-next-line\n",
    "    llm_input_messages: list[AnyMessage]\n",
    "\n",
    "\n",
    "def process_messages(state: MessagesState):\n",
    "    return {\n",
    "        # keep only the last message\n",
    "        \"llm_input_messages\": state[\"messages\"][-1:],\n",
    "        \"some_key\": \"some_value\"\n",
    "    }\n",
    "\n",
    "\n",
    "# highlight-next-line\n",
    "def call_model(state: LLMInputState):\n",
    "    response = model.invoke(state[\"llm_input_messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "model = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "# highlight-next-line\n",
    "builder.add_node(process_messages)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"process_messages\")\n",
    "builder.add_edge(\"process_messages\", \"call_model\")\n",
    "\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# This will now not remember the previous messages\n",
    "# because we set `messages[-1:]` in the filter_messages function\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65737a-dcf9-4987-9de8-948afd6a820d",
   "metadata": {},
   "source": [
    "## Strategies\n",
    "\n",
    "### Filter messages\n",
    "\n",
    "The most straight-forward thing to do to prevent conversation history from blowing up is to filter the list of messages before they get passed to the LLM. This involves two parts: defining a function to filter messages, and then adding it to the graph. See the example below which defines a really simple `filter_messages` function and then uses it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109c1b2-a44e-4ec0-8a11-1377e59315c6",
   "metadata": {},
   "source": [
    "### Trim messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e1df9-da3f-42d2-ace0-6ae8e99be98b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51466c8d-8ce4-4b3d-be4e-18fdbeda5f53",
   "metadata": {},
   "source": [
    "### Delete messages\n",
    "\n",
    "One of the common states for a graph is a list of messages. Usually you only add messages to that state. However, sometimes you may want to remove messages (either by directly modifying the state or as part of the graph). To do that, you can use the `RemoveMessage` modifier. In this guide, we will cover how to do that.\n",
    "\n",
    "The key idea is that each state key has a `reducer` key. This key specifies how to combine updates to the state. The default `MessagesState` has a messages key, and the reducer for that key accepts these `RemoveMessage` modifiers. That reducer then uses these `RemoveMessage` to delete messages from the key.\n",
    "\n",
    "So note that just because your graph state has a key that is a list of messages, it doesn't mean that that this `RemoveMessage` modifier will work. You also have to have a `reducer` defined that knows how to work with this.\n",
    "\n",
    "!!! important \"Valid message history\"\n",
    "\n",
    "    Many models expect certain rules around lists of messages. For example, some expect them to start with a `user` message, others expect all messages with tool calls to be followed by a tool message. **When deleting messages, you will want to make sure you don't violate these rules.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0de5b-30ec-42d4-813a-7ad63fe1c367",
   "metadata": {},
   "source": [
    "### Outside the graph\n",
    "\n",
    "First, we will cover how to manually delete messages. Let's take a look at the current state of the thread:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a850529-d038-48f7-b5a2-8d4d2923f83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}, id='db576005-3a60-4b3b-8925-dc602ac1c571'),\n",
       " AIMessage(content=\"It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. I'm here to help out with any questions or tasks you might have. Please let me know if there's anything I can assist you with.\", additional_kwargs={}, response_metadata={'id': 'msg_01BKAnYxmoC6bQ9PpCuHk8ZT', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 52}}, id='run-3a60c536-b207-4c56-98f3-03f94d49a9e4-0', usage_metadata={'input_tokens': 12, 'output_tokens': 52, 'total_tokens': 64}),\n",
       " HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='2088c465-400b-430b-ad80-fad47dc1f2d6'),\n",
       " AIMessage(content='You said your name is Bob.', additional_kwargs={}, response_metadata={'id': 'msg_013UWTLTzwZi81vke8mMQ2KP', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 72, 'output_tokens': 10}}, id='run-3a6883be-0c52-4938-af98-e9e7476659eb-0', usage_metadata={'input_tokens': 72, 'output_tokens': 10, 'total_tokens': 82})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = app.get_state(config).values[\"messages\"]\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be8a0a-1e94-4302-bd84-d1b72e3c501c",
   "metadata": {},
   "source": [
    "We can call `update_state` and pass in the id of the first message. This will delete that message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1a0970-7e64-4170-beef-2855d10eef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '2',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1ef75157-f251-6a2a-8005-82a86a6593a0'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "app.update_state(config, {\"messages\": RemoveMessage(id=messages[0].id)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9127ae-0d42-42b8-957f-ea69a5da555f",
   "metadata": {},
   "source": [
    "If we now look at the messages, we can verify that the first one was deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bfe4ffa-e170-43bc-aec4-6e36ac620931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I assist you today?\", response_metadata={'id': 'msg_01XPSAenmSqK8rX2WgPZHfz7', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 32}}, id='run-1c69af09-adb1-412d-9010-2456e5a555fb-0', usage_metadata={'input_tokens': 12, 'output_tokens': 32, 'total_tokens': 44}),\n",
       " HumanMessage(content=\"what's my name?\", id='f3c71afe-8ce2-4ed0-991e-65021f03b0a5'),\n",
       " AIMessage(content='Your name is Bob, as you introduced yourself at the beginning of our conversation.', response_metadata={'id': 'msg_01BPZdwsjuMAbC1YAkqawXaF', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 52, 'output_tokens': 19}}, id='run-b2eb9137-2f4e-446f-95f5-3d5f621a2cf8-0', usage_metadata={'input_tokens': 52, 'output_tokens': 19, 'total_tokens': 71})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = app.get_state(config).values[\"messages\"]\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef129a75-4cad-44d7-b532-eb37b0553c0c",
   "metadata": {},
   "source": [
    "### Inside the graph\n",
    "\n",
    "We can also delete messages programmatically from inside the graph. Here we'll modify the graph to delete any old messages (longer than 3 messages ago) at the end of a graph run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265c06b-b35b-47c6-b23a-56966aebdf19",
   "metadata": {},
   "source": [
    "#### TODO: should the delete messages live before or after call model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb22ede0-e153-4fd0-a4c0-f9af2f7663b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight-next-line\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 3:\n",
    "        # highlight-next-line\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:-3]]}\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([delete_messages, call_model])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbdef6-7db7-45a2-8194-de4f8929bd1f",
   "metadata": {},
   "source": [
    "We can now try this out. We can call the graph twice and then check the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3975f34c-c243-40ea-b9d2-424d50a48dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\"), ('ai', 'You said your name is Bob, so that is the name I have for you.')]\n",
      "[('ai', \"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\"), ('human', \"what's my name?\"), ('ai', 'You said your name is Bob, so that is the name I have for you.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2fd2a-14a1-4c47-8632-f8cbb0ba1d35",
   "metadata": {},
   "source": [
    "If we now check the state, we should see that it is only three messages long. This is because we just deleted the earlier messages - otherwise it would be four!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e15abb-81d8-4072-9f10-61ae0fd61dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hello Bob! It's nice to meet you. I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know how I can assist you.\", response_metadata={'id': 'msg_01XPEgPPbcnz5BbGWUDWTmzG', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 48}}, id='run-eded3820-b6a9-4d66-9210-03ca41787ce6-0', usage_metadata={'input_tokens': 12, 'output_tokens': 48, 'total_tokens': 60}),\n",
       " HumanMessage(content=\"what's my name?\", id='a0ea2097-3280-402b-92e1-67177b807ae8'),\n",
       " AIMessage(content='You said your name is Bob, so that is the name I have for you.', response_metadata={'id': 'msg_01JGT62pxhrhN4SykZ57CSjW', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 68, 'output_tokens': 20}}, id='run-ace3519c-81f8-45fe-a777-91f42d48b3a3-0', usage_metadata={'input_tokens': 68, 'output_tokens': 20, 'total_tokens': 88})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = app.get_state(config).values[\"messages\"]\n",
    "messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6089919-e121-45a4-991e-5b3950ba4efb",
   "metadata": {},
   "source": [
    "!!! warning \"Valid message history\"\n",
    "\n",
    "    Remember, when deleting messages you will want to make sure that the remaining message list is still valid. This message list **may actually not be** - this is because it currently starts with an AI message, which some models do not allow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426be57-7156-4455-8dcf-534166435070",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab04149",
   "metadata": {},
   "source": [
    "## Summarize messages\n",
    "\n",
    "One of the most common use cases for persistence is to use it to keep track of conversation history. This is great - it makes it easy to continue conversations. As conversations get longer and longer, however, this conversation history can build up and take up more and more of the context window. This can often be undesirable as it leads to more expensive and longer calls to the LLM, and potentially ones that error. One way to work around that is to create a summary of the conversation to date, and use that with the past N messages. This guide will go through an example of how to do that.\n",
    "\n",
    "This will involve a few steps:\n",
    "\n",
    "- Check if the conversation is too long (can be done by checking number of messages or length of messages)\n",
    "- If yes, the create summary (will need a prompt for this)\n",
    "- Then remove all except the last N messages\n",
    "\n",
    "A big part of this is deleting old messages. For an in depth guide on how to do that, see [this guide](../delete-messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d4279-1ed3-419c-8669-fa96addb69f5",
   "metadata": {},
   "source": [
    "### TODO: replace this completely (use helper?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84835fdb-a5f3-4c90-85f3-0e6257650aba",
   "metadata": {},
   "source": [
    "## Build the chatbot\n",
    "\n",
    "Let's now build the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "378899a9-3b9a-4748-95b6-eb00e0828677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# We will add a `summary` attribute (in addition to `messages` key,\n",
    "# which MessagesState already has)\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# We will use this model for both the conversation and the summarization\n",
    "model = ChatAnthropic(model_name=\"claude-3-haiku-20240307\")\n",
    "\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    # If a summary exists, we add this in as a system message\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# We now define the logic for determining whether to end or summarize the conversation\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    # First, we summarize the conversation\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        # If a summary already exists, we use a different system prompt\n",
    "        # to summarize it than if one didn't\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    # We now need to delete messages that we no longer want to show up\n",
    "    # I will delete all but the last two messages, but you can change this\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the conversation node and the summarize node\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `conversation`.\n",
    "    # This means these are the edges taken after the `conversation` node is called.\n",
    "    \"conversation\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `summarize_conversation` to END.\n",
    "# This means that after `summarize_conversation` is called, we end.\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Finally, we compile it!\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2872e-04b3-4c44-9e03-9e84a5230adf",
   "metadata": {},
   "source": [
    "## Using the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc697132-8fa1-4bf5-9722-56a9859331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_update(update):\n",
    "    for k, v in update.items():\n",
    "        for m in v[\"messages\"]:\n",
    "            m.pretty_print()\n",
    "        if \"summary\" in v:\n",
    "            print(v[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57b27553-21be-43e5-ac48-d1d0a3aa0dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob, as you told me at the beginning of our conversation.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like the celtics!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great, the Celtics are a fun team to follow! Basketball is an exciting sport. Do you have a favorite Celtics player or a favorite moment from a Celtics game you've watched? I'd be happy to discuss the team and the sport with you.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the celtics!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760e219-a7fc-4d81-b4e8-1334c5afc510",
   "metadata": {},
   "source": [
    "We can see that so far no summarization has happened - this is because there are only six messages in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "935265a0-d511-475a-8a0d-b3c3cc5e42a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"hi! I'm bob\", id='6534853d-b8a7-44b9-837b-eb7abaf7ebf7'),\n",
       "  AIMessage(content=\"It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. How can I help you today?\", response_metadata={'id': 'msg_015wCFew2vwMQJcpUh2VZ5ah', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 12, 'output_tokens': 30}}, id='run-0d33008b-1094-4f5e-94ce-293283fc3024-0'),\n",
       "  HumanMessage(content=\"what's my name?\", id='0a4f203a-b95a-42a9-b1c5-bb20f68b3251'),\n",
       "  AIMessage(content='Your name is Bob, as you told me at the beginning of our conversation.', response_metadata={'id': 'msg_01PLp8wg2xDsJbNR9uCtxcGz', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 50, 'output_tokens': 19}}, id='run-3815dd4d-ee0c-4fc2-9889-f6dd40325961-0'),\n",
       "  HumanMessage(content='i like the celtics!', id='ac128172-42d1-4390-b7cc-7bcb2d22ee48'),\n",
       "  AIMessage(content=\"That's great, the Celtics are a fun team to follow! Basketball is an exciting sport. Do you have a favorite Celtics player or a favorite moment from a Celtics game you've watched? I'd be happy to discuss the team and the sport with you.\", response_metadata={'id': 'msg_01CSg5avZEx6CKcZsSvSVXpr', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 78, 'output_tokens': 61}}, id='run-698faa28-0f72-495f-8ebe-e948664d2200-0')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40eddb-9a31-4410-a4c0-9762e2d89e56",
   "metadata": {},
   "source": [
    "Now let's send another message in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "048805a4-3d97-4e76-ac45-8d80d4364c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like how much they win\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite?\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Here is a summary of our conversation so far:\n",
      "\n",
      "- You introduced yourself as Bob and said you like the Boston Celtics basketball team.\n",
      "- I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.\n",
      "- You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.\n",
      "- I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.\n",
      "- The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like how much they win\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b196367-6151-4982-9430-3db7373de06e",
   "metadata": {},
   "source": [
    "If we check the state now, we can see that we have a summary of the conversation, as well as the last two messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09ebb693-4738-4474-a095-6491def5c5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='i like how much they win', id='bb916ce7-534c-4d48-9f92-e269f9dc4859'),\n",
       "  AIMessage(content=\"That's understandable, the Celtics have been one of the more successful NBA franchises over the years. Their history of winning championships is very impressive. It's always fun to follow a team that regularly competes for titles. What do you think has been the key to the Celtics' sustained success? Is there a particular era or team that stands out as your favorite?\", response_metadata={'id': 'msg_01B7TMagaM8xBnYXLSMwUDAG', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 148, 'output_tokens': 82}}, id='run-c5aa9a8f-7983-4a7f-9c1e-0c0055334ac1-0')],\n",
       " 'summary': \"Here is a summary of our conversation so far:\\n\\n- You introduced yourself as Bob and said you like the Boston Celtics basketball team.\\n- I acknowledged that it's nice to meet you, Bob, and noted that you had shared your name earlier in the conversation.\\n- You expressed that you like how much the Celtics win, and I agreed that their history of sustained success and championship pedigree is impressive.\\n- I asked if you have a favorite Celtics player or moment that stands out to you, and invited further discussion about the team and the sport of basketball.\\n- The overall tone has been friendly and conversational, with me trying to engage with your interest in the Celtics by asking follow-up questions.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e4177-c0fc-4fd0-a494-dd03f7f2fddb",
   "metadata": {},
   "source": [
    "We can now resume having a conversation! Note that even though we only have the last two messages, we can still ask it questions about things mentioned earlier in the conversation (because we summarized those)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7094c5ab-66f8-42ff-b1c3-90c8a9468e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In our conversation so far, you introduced yourself as Bob. I acknowledged that earlier when you had shared your name.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40e5db8e-9db9-4ac7-9d76-a99fd4034bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what NFL team do you think I like?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't actually have any information about what NFL team you might like. In our conversation so far, you've only mentioned that you're a fan of the Boston Celtics basketball team. I don't have any prior knowledge about your preferences for NFL teams. Unless you provide me with that information, I don't have a basis to guess which NFL team you might be a fan of.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"what NFL team do you think I like?\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a1a0fda-5309-45f0-9465-9f3dff604d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "i like the patriots!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, got it! Thanks for sharing that you're also a fan of the New England Patriots in the NFL. That makes sense, given your interest in other Boston sports teams like the Celtics. The Patriots have also had a very successful run over the past couple of decades, winning multiple Super Bowls. It's fun to follow winning franchises like the Celtics and Patriots. Do you have a favorite Patriots player or moment that stands out to you?\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "================================\u001b[1m Remove Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Okay, extending the summary with the new information:\n",
      "\n",
      "- You initially introduced yourself as Bob and said you like the Boston Celtics basketball team. \n",
      "- I acknowledged that and we discussed your appreciation for the Celtics' history of winning.\n",
      "- You then asked what your name was, and I reminded you that you had introduced yourself as Bob earlier in the conversation.\n",
      "- You followed up by asking what NFL team I thought you might like, and I explained that I didn't have any prior information about your NFL team preferences.\n",
      "- You then revealed that you are also a fan of the New England Patriots, which made sense given your Celtics fandom.\n",
      "- I responded positively to this new information, noting the Patriots' own impressive success and dynasty over the past couple of decades.\n",
      "- I then asked if you have a particular favorite Patriots player or moment that stands out to you, continuing the friendly, conversational tone.\n",
      "\n",
      "Overall, the discussion has focused on your sports team preferences, with you sharing that you are a fan of both the Celtics and the Patriots. I've tried to engage with your interests and ask follow-up questions to keep the dialogue flowing.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like the patriots!\")\n",
    "input_message.pretty_print()\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"updates\"):\n",
    "    print_update(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd08168-d933-4204-bb93-798a27c202b2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
